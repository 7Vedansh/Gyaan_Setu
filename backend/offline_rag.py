import pickle
import subprocess
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import platform

# =========================
# CONFIG
# =========================
VECTOR_DIR = "vector_store"
EMBED_MODEL = "all-MiniLM-L6-v2"
OLLAMA_MODEL = "phi3"

# Detect OS and set Ollama executable
if platform.system() == "Windows":
    OLLAMA_EXE = r"C:\Users\Ameya Achalla\AppData\Local\Programs\Ollama\ollama.exe"
else:
    OLLAMA_EXE = "ollama"  # Unix/Linux/Mac

TOP_K = 3  # Number of chunks to retrieve

# =========================
# LOAD MODELS & DATA
# =========================
print("üîÅ Loading Offline RAG...")

embedder = SentenceTransformer(EMBED_MODEL)
index = faiss.read_index(f"{VECTOR_DIR}/index.faiss")

with open(f"{VECTOR_DIR}/documents.pkl", "rb") as f:
    documents = pickle.load(f)

print(f"‚úÖ Offline RAG ready with {len(documents)} documents")

# =========================
# LANGUAGE-SPECIFIC PROMPTS
# =========================
RAG_PROMPTS = {
    "en": """You are an offline AI tutor helping students understand science concepts.

CONTEXT FROM TEXTBOOK:
{context}

STUDENT'S QUESTION:
{question}

INSTRUCTIONS:
- Answer ONLY based on the provided context
- Provide a clear, single explanation - no multiple interpretations
- Use proper scientific terminology from the context
- Include relevant formulas if present in the context
- If the answer is NOT in the context, say: "I don't have information about this in my current materials."
- Do NOT repeat statements
- Do NOT add commentary about correctness
- Keep response focused and educational

ANSWER:""",

    "hi": """‡§Ü‡§™ ‡§è‡§ï ‡§ë‡§´‡§≤‡§æ‡§á‡§® AI ‡§ü‡•ç‡§Ø‡•Ç‡§ü‡§∞ ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§õ‡§æ‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§® ‡§ï‡•Ä ‡§Ö‡§µ‡§ß‡§æ‡§∞‡§£‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§

‡§™‡§æ‡§†‡•ç‡§Ø‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§∏‡•á ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠:
{context}

‡§õ‡§æ‡§§‡•ç‡§∞ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®:
{question}

‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:
- ‡§ï‡•á‡§µ‡§≤ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§Ç
- ‡§è‡§ï ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü, ‡§è‡§ï‡§≤ ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç - ‡§ï‡§à ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§è‡§Ç ‡§®‡§π‡•Ä‡§Ç
- ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§∏‡•á ‡§â‡§ö‡§ø‡§§ ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï ‡§∂‡§¨‡•ç‡§¶‡§æ‡§µ‡§≤‡•Ä ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
- ‡§Ø‡§¶‡§ø ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§π‡•ã ‡§§‡•ã ‡§™‡•ç‡§∞‡§æ‡§∏‡§Ç‡§ó‡§ø‡§ï ‡§∏‡•Ç‡§§‡•ç‡§∞ ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§ï‡§∞‡•á‡§Ç
- ‡§Ø‡§¶‡§ø ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§§‡•ã ‡§ï‡§π‡•á‡§Ç: "‡§Æ‡•á‡§∞‡•á ‡§™‡§æ‡§∏ ‡§á‡§∏ ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§"
- ‡§ï‡§•‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§¶‡•ã‡§π‡§∞‡§æ‡§è‡§Ç ‡§®‡§π‡•Ä‡§Ç
- ‡§∏‡§π‡•Ä ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ü‡§ø‡§™‡•ç‡§™‡§£‡•Ä ‡§® ‡§ú‡•ã‡§°‡§º‡•á‡§Ç
- ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§î‡§∞ ‡§∂‡•à‡§ï‡•ç‡§∑‡§ø‡§ï ‡§∞‡§ñ‡•á‡§Ç

‡§â‡§§‡•ç‡§§‡§∞ (‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç):""",

    "mr": """‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§ë‡§´‡§≤‡§æ‡§á‡§® AI ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§Ü‡§π‡§æ‡§§ ‡§ú‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§® ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™‡§®‡§æ ‡§∏‡§Æ‡§ú‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Æ‡§¶‡§§ ‡§ï‡§∞‡§§‡§æ‡§§.

‡§™‡§æ‡§†‡•ç‡§Ø‡§™‡•Å‡§∏‡•ç‡§§‡§ï‡§æ‡§§‡•Ä‡§≤ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠:
{context}

‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®:
{question}

‡§∏‡•Ç‡§ö‡§®‡§æ:
- ‡§´‡§ï‡•ç‡§§ ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§Ü‡§ß‡§æ‡§∞‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ
- ‡§è‡§ï ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü, ‡§è‡§ï‡§≤ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü‡•Ä‡§ï‡§∞‡§£ ‡§¶‡•ç‡§Ø‡§æ - ‡§Ö‡§®‡•á‡§ï ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ ‡§®‡§æ‡§π‡•Ä‡§§
- ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§§‡•Ç‡§® ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï ‡§∂‡§¨‡•ç‡§¶‡§æ‡§µ‡§≤‡•Ä ‡§µ‡§æ‡§™‡§∞‡§æ
- ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§§ ‡§Ö‡§∏‡§≤‡•ç‡§Ø‡§æ‡§∏ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§∏‡•Ç‡§§‡•ç‡§∞‡•á ‡§∏‡§Æ‡§æ‡§µ‡§ø‡§∑‡•ç‡§ü ‡§ï‡§∞‡§æ
- ‡§ú‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§§ ‡§®‡§∏‡•á‡§≤, ‡§§‡§∞ ‡§Æ‡•ç‡§π‡§£‡§æ: "‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ‡§ï‡§°‡•á ‡§∏‡§ß‡•ç‡§Ø‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§∏‡§æ‡§π‡§ø‡§§‡•ç‡§Ø‡§æ‡§§ ‡§Ø‡§æ‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§®‡§æ‡§π‡•Ä."
- ‡§µ‡§ø‡§ß‡§æ‡§®‡•á ‡§™‡•Å‡§®‡§∞‡§æ‡§µ‡•É‡§§‡•ç‡§§‡•Ä ‡§ï‡§∞‡•Ç ‡§®‡§ï‡§æ
- ‡§Ø‡•ã‡§ó‡•ç‡§Ø‡§§‡•á‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§≠‡§æ‡§∑‡•ç‡§Ø ‡§ú‡•ã‡§°‡•Ç ‡§®‡§ï‡§æ
- ‡§™‡•ç‡§∞‡§§‡§ø‡§∏‡§æ‡§¶ ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§Ü‡§£‡§ø ‡§∂‡•à‡§ï‡•ç‡§∑‡§£‡§ø‡§ï ‡§†‡•á‡§µ‡§æ

‡§â‡§§‡•ç‡§§‡§∞ (‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§):"""
}

# =========================
# RETRIEVE CONTEXT
# =========================
def retrieve_context(question: str, k: int = TOP_K) -> tuple[str, float]:
    """
    Retrieve relevant context from vector store.
    
    Returns:
        (context_text, confidence_score)
    """
    query_embedding = embedder.encode([question])
    distances, indices = index.search(np.array(query_embedding), k)
    
    # Calculate confidence based on similarity
    # Lower L2 distance = higher similarity
    avg_distance = float(np.mean(distances[0]))
    confidence = max(0.0, min(1.0, 1.0 / (1.0 + avg_distance)))
    
    retrieved_chunks = []
    for idx in indices[0]:
        if idx < len(documents):  # Safety check
            retrieved_chunks.append(documents[idx]["content"])
    
    context = "\n\n---\n\n".join(retrieved_chunks)
    return context, confidence


# =========================
# CLEAN OUTPUT
# =========================
def clean_output(text: str) -> str:
    """
    Remove duplicate lines and excessive whitespace.
    """
    lines = text.split("\n")
    unique_lines = []
    seen = set()
    
    for line in lines:
        stripped = line.strip()
        # Keep line if non-empty and not seen before
        if stripped and stripped not in seen:
            unique_lines.append(line)
            seen.add(stripped)
    
    return "\n".join(unique_lines).strip()


# =========================
# MAIN FUNCTION
# =========================
def run_offline_rag(question: str, language: str) -> str:
    """
    Generate answer using offline RAG.
    
    Args:
        question: Student's question
        language: Language code ('en', 'hi', 'mr')
    
    Returns:
        Answer based on retrieved context
    """
    # Validate language
    if language not in RAG_PROMPTS:
        print(f"‚ö† Unsupported language '{language}', defaulting to English")
        language = "en"
    
    # Retrieve context
    context, confidence = retrieve_context(question)
    
    if not context:
        return "I don't have any relevant information in my materials."
    
    # Build prompt
    prompt = RAG_PROMPTS[language].format(
        context=context,
        question=question
    )
    
    try:
        # Run Ollama
        process = subprocess.Popen(
            [OLLAMA_EXE, "run", OLLAMA_MODEL],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        stdout, stderr = process.communicate(prompt.encode("utf-8"), timeout=30)
        
        if process.returncode != 0:
            print(f"‚ùå Ollama error: {stderr.decode('utf-8', errors='ignore')}")
            return "Unable to generate response from offline model."
        
        raw_answer = stdout.decode("utf-8", errors="ignore").strip()
        
        # Clean output
        final_answer = clean_output(raw_answer)
        
        return final_answer if final_answer else "Unable to generate a proper response."
        
    except subprocess.TimeoutExpired:
        print("‚ùå Ollama timeout")
        process.kill()
        return "Response generation timed out."
    except Exception as e:
        print(f"‚ùå Offline RAG error: {e}")
        return f"Error generating offline response: {str(e)}"